{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahupra1357/TensorflowCertification/blob/main/3_NLP/Copy_of_C3_W1_Lab_1_tokenize_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRYl9MwwzY_e"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C3/W1/ungraded_labs/C3_W1_Lab_1_tokenize_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL-LzAqpoGLC"
      },
      "source": [
        "# Ungraded Lab: Tokenizer Basics\n",
        "\n",
        "In most NLP tasks, the initial step in preparing your data is to extract a vocabulary of words from your *corpus* (i.e. input texts). You will need to define how to represent the texts into numerical representations which can be used to train a neural network. These representations are called *tokens* and Tensorflow and Keras makes it easy to generate these using its APIs. You will see how to do that in the next cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nt3uR9TPrUt"
      },
      "source": [
        "## Generating the vocabulary\n",
        "\n",
        "In this notebook, you will look first at how you can provide a look up dictionary for each word. The code below takes a list of sentences, then takes each word in those sentences and assigns it to an integer. This is done using the [fit_on_texts()](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#fit_on_texts) method and you can get the result by looking at the `word_index` property. More frequent words have a lower index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaCMcjMQifQc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a9757cc-b460-4e4a-9a0d-4ca1daf70b0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Define input sentences\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat'\n",
        "    ]\n",
        "\n",
        "# Initialize the Tokenizer class\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "\n",
        "# Generate indices for each word in the corpus\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Get the indices and print it\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTPWesNaRdX2"
      },
      "source": [
        "The `num_words` parameter used in the initializer specifies the maximum number of words minus one (based on frequency) to keep when generating sequences. You will see this in a later exercise. For now, the important thing to note is it does not affect how the `word_index` dictionary is generated. You can try passing `1` instead of `100` as shown on the next cell and you will arrive at the same `word_index`.\n",
        "\n",
        "Also notice that by default, all punctuation is ignored and words are converted to lower case. You can override these behaviors by modifying the `filters` and `lower` arguments of the `Tokenizer` class as described [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#arguments). You can try modifying these in the next cell below and compare the output to the one generated above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VX1A1pDNoVKm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "441ea15f-a90b-442b-fc49-627105fc3cb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ]
        }
      ],
      "source": [
        "# Define input sentences\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat',\n",
        "    'You love my dog!'\n",
        "]\n",
        "\n",
        "# Initialize the Tokenizer class\n",
        "tokenizer = Tokenizer(num_words = 1)\n",
        "\n",
        "# Generate indices for each word in the corpus\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Get the indices and print it\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9LFfwBffDaj"
      },
      "source": [
        "That concludes this short exercise on tokenizing input texts!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define your input texts\n",
        "sentences = [\n",
        "    'I love my dog',\n",
        "    'I love my cat',\n",
        "    'You love my dog!',\n",
        "    'Do you think my dog is amazing?'\n",
        "]\n",
        "\n",
        "# Initialize the Tokenizer class\n",
        "tokenizer = Tokenizer(num_words = 6)\n",
        "\n",
        "# Tokenize the input sentences\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Get the word index dictionary\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Generate list of token sequences\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\nWord Index = \" , word_index)\n",
        "print(\"\\nSequences = \" , sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHSBiVc45qEh",
        "outputId": "1650f83f-3723-401e-828d-a1484038c75f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Index =  {'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
            "\n",
            "Sequences =  [[4, 2, 1, 3], [4, 2, 1], [5, 2, 1, 3], [5, 1, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define your input texts\n",
        "sentences = [\n",
        "    'I love my dog',\n",
        "    'I love my cat',\n",
        "    'AAA I love my cat',\n",
        "    'You love my dog!',\n",
        "    'Do you think my dog is amazing?'\n",
        "]\n",
        "\n",
        "# Initialize the Tokenizer class\n",
        "tokenizer = Tokenizer(num_words = 6)\n",
        "\n",
        "# Tokenize the input sentences\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Get the word index dictionary\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Generate list of token sequences\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\nWord Index = \" , word_index)\n",
        "print(\"\\nSequences = \" , sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSzOm8nB6KAz",
        "outputId": "eb464a78-e63e-49e1-941f-ba21469285e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Index =  {'my': 1, 'love': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6, 'aaa': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "\n",
            "Sequences =  [[3, 2, 1, 4], [3, 2, 1, 5], [3, 2, 1, 5], [2, 1, 4], [1, 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad the sequences to a uniform length\n",
        "padded = pad_sequences(sequences, maxlen=7)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\nPadded Sequences:\")\n",
        "print(padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7MZtu_K6v-1",
        "outputId": "2d9c0414-e0fe-46a9-9a65-22e83f247685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Padded Sequences:\n",
            "[[0 0 0 3 2 1 4]\n",
            " [0 0 0 3 2 1 5]\n",
            " [0 0 0 3 2 1 5]\n",
            " [0 0 0 0 2 1 4]\n",
            " [0 0 0 0 0 1 4]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}